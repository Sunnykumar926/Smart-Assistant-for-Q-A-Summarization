# ğŸ¤– Smart Assistant for Q&A and Summarization

## ğŸ¥ Demo Video

[â–¶ï¸ Watch the Demo on Loom](https://www.loom.com/share/50a709f000e54135bf61690752936a9b)

A **Streamlit-based GenAI assistant** that reads and understands content from `.pdf` and `.txt` documents. It performs **contextual Q&A**, **logic-based question generation**, and **source-grounded evaluation**, powered by **Google Gemini 1.5 Flash** and **LangChain**.

> ğŸ“ Upload research papers, legal documents, or technical reports â€” let the assistant read, reason, and quiz you!

---

## ğŸ¯ Project Objective

This project was built as part of a **GenAI recruitment challenge** by **EZLab**, aiming to develop an AI-powered assistant capable of:

- ğŸ“š Understanding complex documents  
- â“ Answering free-form questions with citations  
- ğŸ§  Generating logic-based questions for users  
- âœ… Evaluating user answers with constructive feedback  
- ğŸ” Justifying all responses using document-grounded evidence  

---

## ğŸš€ Key Features

- ğŸ“„ **Multi-format Upload**: Accepts `.pdf` and `.txt` files  
- âœï¸ **Auto-Summarization**: Concise <150-word summary per document  
- ğŸ’¬ **Ask Anything**: Conversational Q&A with supporting quotes  
- ğŸ§  **Challenge Mode**:  
  - Generates logic/reasoning-based questions  
  - Evaluates answers and provides feedback  
- ğŸ“š **Document Grounding**: Every answer includes exact sources; no hallucinations  

---

## ğŸ§  How It Works

### ğŸ“ Document Parsing
- Uses **PyPDF2** and UTF-8 decoding to extract structured content  
- Retains page numbers and source names for traceability  

### ğŸ” Vector Store Creation
- Uses `RecursiveCharacterTextSplitter` for efficient chunking  
- Embeds chunks using **Google GenerativeAI Embeddings**  
- Stores vectors locally using **FAISS**  

### ğŸ’¬ Q&A & Feedback Loop
- Uses `ChatGoogleGenerativeAI` (Gemini 1.5 Flash) via LangChain  
- Employs a structured prompt for grounded Q&A  
- Evaluates user answers with rubric-based criteria (accuracy, completeness, citation)  

---

## ğŸ—ï¸ Tech Stack

| Layer         | Technology Used                         |
|---------------|------------------------------------------|
| UI            | Streamlit                                |
| LLM Backend   | Google Gemini 1.5 Flash (`langchain-google-genai`) |
| Vector Store  | FAISS                                     |
| Parsing       | PyPDF2, UTF-8 Decoding                   |
| Environment   | `python-dotenv`                          |
| Chaining Logic| LangChain                                |

---

## ğŸ“¦ Installation

### ğŸ§° Prerequisites

- Python 3.8+
- Google Gemini API Key (add to `.env` as `GOOGLE_API_KEY`)

### ğŸ”§ Setup
1. Clone repository:
```bash
git clone https://github.com/Sunnykumar926/Smart-Assistant-for-Research-Summarization
```
2. Create virtual environment:
```bash
python -m venv my_env
source my_env/bin/activate  # Linux/MacOS
my_env\Scripts\activate    # Windows
```
3. Install dependencies:
```bash
pip install -r requirements.txt
```
4. Create `.env` file with your API key:
```bash
GOOGLE_API_KEY=your_api_key_here
```
5. Run your file
```bash
streamlit run app.py
```
## ğŸ” Workflow Diagram

```mermaid
graph TD
    A[ğŸ“‚ Document Upload] --> B[ğŸ“ Text Extraction]
    B --> C[ğŸ”— Chunk Splitting]
    C --> D[ğŸ” Vector Embedding]
    D --> E[ğŸ—‚ï¸ FAISS Storage]
    E --> F{ğŸ¤– User Interaction}
    F --> G[ğŸ’¬ Question Answering]
    F --> H[ğŸ§  Knowledge Validation]
    G --> I[ğŸ“ Response Generation]
    H --> J[â“ Question Creation]
    J --> K[âœ… Answer Evaluation]
```
---
## ğŸ‘¨â€ğŸ’» Author

**Sunny Kumar**  
âœ¨ Developed for EZLab's GenAI task








